# 信息论入门教程

1948年，美国数学家克劳德·香农发表论文《通信的数学理论》（A Mathematical Theory of Communication），奠定了信息论的基础。

今天，信息论在信号处理、数据压缩、自然语言处理等许多领域，有着重要的应用。虽然，它的数学形式很复杂，但是核心思想非常简单，只要中学数学水平就能理解。

本文使用一个最简单的例子，帮助大家理解信息论。

## 一、词汇的编码

小张是我的好朋友，最近去了美国。我们保持着邮件联系。

为了简化问题，小张写信的时候，假定只使用4个词汇：狗，猫，鱼，鸟。

信的所有内容就是这4个词的组合。举例来说，第一封信写着“狗猫鱼鸟”，第二封信写“鱼猫鸟狗”。

这些信件经过二进制编码以后，在互联网上传递。四个词汇的最小编码方式，是只使用两个二进制位。

> - 狗 00
> - 猫 01
> - 鱼 10
> - 鸟 11

所以，第一封信“狗猫鱼鸟”的编码是`00011011`，第二封信“鱼猫鸟狗”的编码是`10011100`。

如果每个词汇的出现概率是独立的，而且均衡分布，那么上面的编码就是最短编码方式，不可能找出更短的编码。后文会解释原因。

## 二、词汇的分布概率

最近，小张开始养狗，他的信里提到狗的次数，多于其他词汇。词汇分布的概率因此不是均衡的，假定概率分布如下。

- 狗：50%
- 猫：25%
- 鱼：12.5%
- 鸟：12.5%

现在，小张写了一封信，符合上面的概率分布。

> 狗狗狗狗猫猫鱼鸟

上面这封信，用前一节的方法进行编码。

> 0000000001011011

一共需要16个二进制。我们的问题就是，这是这封信的最短编码方式吗？

回答是否定的。因为“狗”的出现次数多于其他词，所以可以给它分配更短的编码，那些少见的词分配更长的编码。请看下面的编码方式。

> - 狗 0
> - 猫 10
> - 鱼 110
> - 鸟 111

使用新的编码方式，小张的信只需要14个二进制位。

> 00001010110111

平均计算，每个词只需要1.75个二进制位（14 / 8），后文同样可以证明，这是最短的编码方式。

## 三、编码方式的唯一性

前一节的编码方式，狗的编码是`0`，这里的问题是，可以把这个编码改成`1`吗，即下面的编码可行吗？

> - 狗 1
> - 猫 10
> - 鱼 110
> - 鸟 111

回答是否定的。如果狗的编码是`1`，会造成无法解码，即解码结果不唯一。`110`有可能是“狗猫”，也可能是“鱼”。现在的编码方式，其实是唯一不会造成歧义的编码。

下面是数学的证明。一个二进制位可以表示两种可能性：`0`和`1`。如果可能性多于两种（比如本例是四种可能），就只能让一个二进制位拥有特殊含义，另一个二进制位必须空出来，保证能够唯一解码。比如，`0`表示狗，`1`就必须空出来，不能有特殊含义。

同理，两个二进制位可以表示四种可能性：`00`、`01`、`10`和`11`。在上面的例子中，为了避免无法解码，`0`开头的编码不再用了，只剩下`10`和`11`可用。由于本例存在四种可能性，所以只能用`10`表示猫，将`11`空出来，后面用三个二进制表示鱼和鸟。

综上所述，当分布不均衡时，

- 如果两种可能，每种需要一个二进制位表示。
- 如果三种可能，每种需要一到两个二进制位表示。
- 如果四种可能，每种需要一到三个二进制位表示。

## 四、编码与概率的关系

上一节的讨论，还可以得到一个结论：**概率越大，所需要的二进制位越少。**

- 狗的概率是50%，表示每两个词汇里面，就有一个是狗，因此单独分配给它1个二进制位。
- 猫的概率是25%，分配给它两个二进制位。
- 鱼和鸟的概率是12.5%，分配给它们三个二进制位。

香农的论文给出了一般性的数学公式。`L`表示所需要的二进制位，`p(x)`表示发生的概率，它们的关系如下。

> L = log₂(1 / p(x))

举例来说，概率`0.125`的倒数为`8`， 以 2 为底的对数就是`3`，表示需要三个二进制位。

对于一种概率分布来说，每种概率需要的编码长度，乘以对应的概率，再求和，就可以得到该分布的平均编码长度。

> 

上面公式的`H`，就是该种概率分布的每种可能性的平均编码长度。理论上，这也是最优编码长度，不可能获得比它更短的编码了。

还是用上面的例子，如果“狗猫鱼鸟”是均衡分布，每个词平均需要2个二进制位。

> H = 0.25 x 2 + 0.25  x 2 + 0.25  x 2 + 0.25 x 2
>     = 2

如果“狗猫鱼鸟”不是均匀分布，每个词平均需要1.75个二进制位。

> H = 0.5 x 1 + 0.25 x 2 + 0.125 x 3 + 0.125 x 3
>     = 1.75

每个词平均需要 1.75 个二进制位，所以，“狗狗狗狗猫猫鱼鸟”这8个词的句子，需要14个二进制位（8 x 1.75）。

很显然，不均匀分布时，某个词出现的概率越高，编码后内容就更短。出现的概率高，表示重复的次数多，因此推论就是，重复内容越多，越容易得到好的压缩效果。

## 五、信息熵

上一节公式的`H`（平均编码长度），又称为“信息熵”（information entroy）。

`H`越大，表示每种结果的概率越小。概率越小，则表示可能的结果越多，不确定性越高。这也是“信息熵”这个名字的来源，在物理学里，熵就表示无序，越无序的状态，熵越高。

比如，`H`为`1`，表示只需要一个二进制位，就能表示所有可能性，即只可能有两种结果。如果`H`为`6`，六个二进制位表示有64种可能性，不确定性大大提高。

请看这个例子，如果一个人的词汇量为10万，意味着每个词有10万种可能，均匀分布时，每个词需要 16.61 个二进制位。

> log₂(100, 000) = 16.61

所以，你写一篇1000个词的文章，需要 1.6 万个二进制位。

> 16.61 x 1000 = 16, 610

相比之下，一幅像素密度为 480 x 640、16级灰度的图片，需要123万个二进制位。

> 480 x 640 x log₂(16) =  1, 228, 800

所以一幅图片所能传递的信息远远超过文字，这就是“一图胜千言”吧。

上面假定，文章和图片都是均匀分布，现实生活中，一般都是不均衡分布，所以上面计算的二进制位长度是最大长度，根据不均匀分布的程序，可以进行压缩。

## 六、参考链接

- [Visual Information Theory](https://colah.github.io/posts/2015-09-Visual-Information/), by Christopher Olah
- [Information Theory (PDF)](https://www.cs.cmu.edu/~roni/10601-slides/info-theory-x4.pdf), Roni Rosenfeld

（完）

